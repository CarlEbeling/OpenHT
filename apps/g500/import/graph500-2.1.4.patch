diff -u /dev/null graph500-2.1.4/convey-csr/convey-csr.c
--- /dev/null	1969-12-31 18:00:00.000000000 -0600
+++ graph500-2.1.4/convey-csr/convey-csr.c	2015-03-05 10:30:14.288926000 -0600
@@ -0,0 +1,502 @@
+/* Copyright 2010,  Georgia Institute of Technology, USA. */
+/* See COPYING for license. */
+#include <stdlib.h>
+#include <stdio.h>
+#include <stdint.h>
+#include <string.h>
+#include <math.h>
+
+#include <assert.h>
+#include <alloca.h>
+#include <omp.h>
+#include <pthread.h>
+
+#include "convey-csr.h"
+#include "../timer.h"
+#include "../compat.h"
+
+extern void g500_hyb_wrapper ( int64_t g500_ctl, int64_t nv,
+               int64_t *bfs_tree,
+	       XADJ_SIZE *xadj, int64_t *xoff,
+               int64_t *bfs_packed_cp,
+               int64_t *bfs_bit_map_cp, int64_t *bfs_bit_map_new_cp,
+               int64_t level, int64_t *k1, int64_t *k2);
+
+static int64_t int64_fetch_add (int64_t* p, int64_t incr);
+static int64_t int64_casval(int64_t* p, int64_t oldval, int64_t newval);
+static int int64_cas(int64_t* p, int64_t oldval, int64_t newval);
+
+double bottom_threshold();
+
+void top_hybrid (int64_t nv, int64_t srcvtx, double bottom_constant,
+  int64_t *k1, int64_t *k2, 
+  int64_t *level, 
+  int64_t *bfs_tree, int64_t *vlist,
+  int64_t *bfs_packed_host, int64_t *vlist_host, 
+  int64_t *xoff, XADJ_SIZE *xadj, 
+  int64_t *bfs_bit_map_host, int64_t *bfs_bit_map_cp);
+
+void host_processing_openmp(int64_t nv, int64_t srcvtx,
+  int64_t *k1, int64_t *k2, int64_t *level,
+  int64_t *bfs_packed_host, int64_t *vlist_host,
+  int64_t *xoff, XADJ_SIZE *xadj, int64_t *bfs_bit_map_host);
+
+
+extern double walltime();
+
+#include "../graph500.h"
+#include "../xalloc.h"
+#include "../generator/graph_generator.h"
+
+#define MINVECT_SIZE 2
+
+static int64_t maxvtx, nv, sz;
+int64_t * restrict xoff; /* Length 2*nv+2 */
+int64_t * restrict cny_xoff; /* Length 2*nv+2 */
+
+
+XADJ_SIZE * restrict xadjstore; /* Length MINVECT_SIZE + (xoff[nv] == nedge) */
+XADJ_SIZE * restrict xadj;
+XADJ_SIZE * restrict cny_xadjstore; /* Length MINVECT_SIZE + (xoff[nv] == nedge) */
+XADJ_SIZE * restrict cny_xadj;
+
+static void
+find_nv (const struct packed_edge * restrict IJ, const int64_t nedge)
+{
+  maxvtx = -1;
+  OMP("omp parallel") {
+    int64_t k, gmaxvtx, tmaxvtx = -1;
+
+    OMP("omp for")
+      for (k = 0; k < nedge; ++k) {
+	if (get_v0_from_edge(&IJ[k]) > tmaxvtx)
+	  tmaxvtx = get_v0_from_edge(&IJ[k]);
+	if (get_v1_from_edge(&IJ[k]) > tmaxvtx)
+	  tmaxvtx = get_v1_from_edge(&IJ[k]);
+      }
+    gmaxvtx = maxvtx;
+    while (tmaxvtx > gmaxvtx)
+      gmaxvtx = int64_casval (&maxvtx, gmaxvtx, tmaxvtx);
+  }
+  nv = 1+maxvtx;
+}
+
+static int
+alloc_graph (int64_t nedge)
+{
+  sz = (2*nv+2) * sizeof (*xoff);
+  xoff = xmalloc_large_ext (sz);
+  if (!xoff) return -1;
+
+  cny_xoff = pers_cp_malloc (sz);
+  if (!cny_xoff) return -1;
+
+  return 0;
+}
+
+static void
+free_graph (void)
+{
+
+  xfree_large (xadjstore);
+  xfree_large (xoff);
+
+  pers_cp_free (cny_xadjstore);
+  pers_cp_free (cny_xoff);
+
+}
+
+#define XOFF(k) (xoff[2*(k)])
+#define XENDOFF(k) (xoff[1+2*(k)])
+
+static int64_t
+prefix_sum (int64_t *buf)
+{
+  int nt, tid;
+  int64_t slice_begin, slice_end, t1, t2, k;
+
+  nt = omp_get_num_threads ();
+  tid = omp_get_thread_num ();
+
+  t1 = nv / nt;
+  t2 = nv % nt;
+  slice_begin = t1 * tid + (tid < t2? tid : t2);
+  slice_end = t1 * (tid+1) + ((tid+1) < t2? (tid+1) : t2);
+
+  buf[tid] = 0;
+  for (k = slice_begin; k < slice_end; ++k)
+    buf[tid] += XOFF(k);
+  OMP("omp barrier");
+  OMP("omp single")
+    for (k = 1; k < nt; ++k)
+      buf[k] += buf[k-1];
+  if (tid)
+    t1 = buf[tid-1];
+  else
+    t1 = 0;
+  for (k = slice_begin; k < slice_end; ++k) {
+    int64_t tmp = XOFF(k);
+    XOFF(k) = t1;
+    t1 += tmp;
+  }
+  OMP("omp flush (xoff)");
+  OMP("omp barrier");
+  return buf[nt-1];
+}
+
+static int
+setup_deg_off (const struct packed_edge * restrict IJ, int64_t nedge)
+{
+  int err = 0;
+  int64_t *buf = NULL;
+  xadj = NULL;
+  OMP("omp parallel") {
+    int64_t k, accum;
+    OMP("omp for")
+      for (k = 0; k < 2*nv+2; ++k)
+	xoff[k] = 0;
+    OMP("omp for")
+      for (k = 0; k < nedge; ++k) {
+        int64_t i = get_v0_from_edge(&IJ[k]);
+        int64_t j = get_v1_from_edge(&IJ[k]);
+	if (i != j) { /* Skip self-edges. */
+	  if (i >= 0)
+	    OMP("omp atomic")
+	      ++XOFF(i);
+	  if (j >= 0)
+	    OMP("omp atomic")
+	      ++XOFF(j);
+	}
+      }
+    OMP("omp single") {
+      buf = alloca (omp_get_num_threads () * sizeof (*buf));
+      if (!buf) {
+	perror ("alloca for prefix-sum hosed");
+	abort ();
+      }
+    }
+    OMP("omp for")
+      for (k = 0; k < nv; ++k)
+	if (XOFF(k) < MINVECT_SIZE) XOFF(k) = MINVECT_SIZE;
+
+    accum = prefix_sum (buf);
+
+    OMP("omp for")
+      for (k = 0; k < nv; ++k)
+	XENDOFF(k) = XOFF(k);
+
+    OMP("omp single") {
+      XOFF(nv) = accum;
+      if (!(xadjstore = xmalloc_large_ext ((XOFF(nv) + MINVECT_SIZE) * sizeof (*xadjstore))))
+	err = -1;
+      if (!err) {
+	xadj = &xadjstore[MINVECT_SIZE]; /* Cheat and permit xadj[-1] to work. */
+  	for (k = 0; k < XOFF(nv) + MINVECT_SIZE; ++k)
+	  xadjstore[k] = -1;
+      }
+
+      if (!(cny_xadjstore = pers_cp_malloc ((XOFF(nv) + MINVECT_SIZE) * sizeof (*cny_xadjstore))))
+        err = -1;
+      if (!err) {
+        cny_xadj = &cny_xadjstore[MINVECT_SIZE]; /* Cheat and permit xadj[-1] to work. */
+      } else {
+        xfree_large (xadjstore);
+      }
+
+    }
+  }
+
+  return !xadj;
+}
+
+static void
+scatter_edge (const int64_t i, const int64_t j)
+{
+  int64_t where;
+  where = int64_fetch_add (&XENDOFF(i), 1);
+  xadj[where] = j;
+}
+
+static int
+i64cmp (const void *a, const void *b)
+{
+#if defined (XADJ_32)
+  const int ia = *(const int*)a;
+  const int ib = *(const int*)b;
+#else
+  const int64_t ia = *(const int64_t*)a;
+  const int64_t ib = *(const int64_t*)b;
+#endif
+  if (ia < ib) return -1;
+  if (ia > ib) return 1;
+  return 0;
+}
+
+static void
+pack_vtx_edges (const int64_t i)
+{
+  int64_t kcur, k;
+  if (XOFF(i)+1 >= XENDOFF(i)) return;
+  qsort (&xadj[XOFF(i)], XENDOFF(i)-XOFF(i), sizeof(*xadj), i64cmp);
+  kcur = XOFF(i);
+  for (k = XOFF(i)+1; k < XENDOFF(i); ++k)
+    if (xadj[k] != xadj[kcur])
+      xadj[++kcur] = xadj[k];
+  ++kcur;
+  for (k = kcur; k < XENDOFF(i); ++k)
+    xadj[k] = -1;
+  XENDOFF(i) = kcur;
+}
+
+static void
+pack_edges (void)
+{
+  int64_t v;
+
+  OMP("omp for")
+    for (v = 0; v < nv; ++v)
+      pack_vtx_edges (v);
+}
+
+static void
+gather_edges (const struct packed_edge * restrict IJ, int64_t nedge)
+{
+  OMP("omp parallel") {
+    int64_t k;
+
+    OMP("omp for")
+      for (k = 0; k < nedge; ++k) {
+        int64_t i = get_v0_from_edge(&IJ[k]);
+        int64_t j = get_v1_from_edge(&IJ[k]);
+	if (i >= 0 && j >= 0 && i != j) {
+	  scatter_edge (i, j);
+	  scatter_edge (j, i);
+	}
+      }
+
+    pack_edges ();
+  }
+}
+
+int 
+create_graph_from_edgelist (struct packed_edge *IJ, int64_t nedge)
+{
+  find_nv (IJ, nedge);
+
+  if (alloc_graph (nedge)) return -1;
+  if (setup_deg_off (IJ, nedge)) {
+    xfree_large (xoff);
+    pers_cp_free (cny_xoff);
+    return -1;
+  }
+
+  gather_edges (IJ, nedge);
+
+// copy migrate data to CP
+  static int64_t sz, sz_xadjstore;
+
+  sz = (2*nv+2);
+  sz_xadjstore = XOFF(nv) + MINVECT_SIZE;
+
+// copy data to CP
+  pers_cp_memcpy(cny_xoff, xoff, (sz * sizeof (*xoff)));
+  pers_cp_memcpy(cny_xadjstore, xadjstore, (sz_xadjstore * sizeof (*xadjstore)));
+
+  return 0;
+}
+
+
+int
+make_bfs_tree (int64_t *bfs_tree_out_host, int64_t *bfs_tree_out, 
+               int64_t *max_vtx_out, int64_t srcvtx)
+{
+
+  int64_t k1, k2;
+  int64_t level = 0;
+
+  int64_t bit_map_size;
+  int64_t g500_ctl;
+  double bottom_constant;
+
+  double t0;
+  double copy_bfs_vlist_bit_time;
+  double one_gb_pts = 1024.*1024.*1024.;
+
+  int err = 0;
+
+// must calculate bit_map_size before following packed mapping
+/*
+  bit_map_size = nv / 64;
+  if ( nv != 64*bit_map_size)
+    bit_map_size++;
+*/
+int64_t one   = 0x0000000000000001;
+int64_t shift6 = 6;
+int64_t and63_c = 0xffffffffffffff80;
+
+#define BIT_LENGTH(n) ( (n) == ( (n) & and63_c) ? ((n) >> shift6 ) : ((n >> shift6 ) + one) )
+  bit_map_size = BIT_LENGTH(nv);
+
+  int64_t * restrict bfs_bit_map_host = bfs_tree_out_host;
+  int64_t * restrict bfs_packed_host = &bfs_tree_out_host[bit_map_size];
+  int64_t * restrict vlist_host = &bfs_tree_out_host[bit_map_size+1];
+
+  int64_t * restrict bfs_tree = bfs_tree_out;
+  int64_t * restrict bfs_bit_map_cp = &bfs_tree_out[nv];
+  int64_t * restrict bfs_packed_cp = &bfs_tree_out[nv+bit_map_size];
+  int64_t * restrict vlist = &bfs_tree_out[nv+bit_map_size+1];
+  int64_t * restrict bfs_bit_map_new_cp = &bfs_tree_out[2*nv+bit_map_size+1];
+
+  *max_vtx_out = maxvtx;
+
+// touch the first and last points of bfs_bit_map_host
+  bfs_bit_map_host[0] = -1;
+  bfs_bit_map_host[bit_map_size-1] = -1;
+
+// touch the first and last points of bfs_tree_cp
+  bfs_tree[0] = -1;
+  bfs_tree[nv-1] = -1;
+
+/////////////////////////////////////////////////////////////////
+// use the system type to set the threshold
+  bottom_constant = bottom_threshold();
+#if defined (TIMER_ON)
+  printf ("bottom constant for top/down threshold %f \n",bottom_constant);
+#endif // TIMER_ON
+
+  top_hybrid (nv, srcvtx, bottom_constant, &k1, &k2, &level,
+    bfs_tree, vlist,
+    bfs_packed_host, vlist_host, xoff, xadj, 
+    bfs_bit_map_host, bfs_bit_map_cp);
+
+//////////////////////////////////////////////////////////////
+
+// G500 control register: 
+//   to distinguish between 32 and 64-bit edges
+//
+//   ctl[6-0]  = 1000000 for 64-bit edges
+//   ctl[6-0]  = 0100000 for 32-bit edges
+
+// 9-7-2011 removing single_ae logic since no longer needed
+
+#if defined (XADJ_32)
+// 4 ae, 32 bit xadj (edges)
+  g500_ctl = 32;
+  static int once;
+  if (!once++) printf ("Using int32_t XADJ\n");
+#else
+// 4 ae, 64 bit xadj (edges)
+  g500_ctl = 64;
+#endif
+
+// bits 12:8 are no longer used
+  int64_t max_num_reqs_per_thread=1;
+// OR in the MAX_NUM_REQS/thread to bits 12:8 of g500_ctl
+  g500_ctl = (g500_ctl | (max_num_reqs_per_thread <<8));
+
+// Number of threads per pipe to use.  HW default = max = 512 
+  int64_t num_threads_per_pipe=512;
+// OR in the num_threads_per_pipe to bits 24:16 of g500_ctl
+  g500_ctl = (g500_ctl | (num_threads_per_pipe <<16));
+
+  g500_hyb_wrapper ( g500_ctl, nv, bfs_tree, cny_xadj, cny_xoff, 
+    bfs_packed_cp, bfs_bit_map_cp, bfs_bit_map_new_cp, level, &k1, &k2 );
+
+  return err;
+}
+
+////////////////////////////////////////////////////////////////
+
+void
+destroy_graph (void)
+{
+  free_graph ();
+}
+
+#if defined(_OPENMP)
+#if defined(__GNUC__)||defined(__INTEL_COMPILER)
+// debug - remove later
+//#if ((defined(__GNUC__)||defined(__INTEL_COMPILER)) && (!defined(__CONVEY)))
+int64_t
+int64_fetch_add (int64_t* p, int64_t incr)
+{
+  return __sync_fetch_and_add (p, incr);
+}
+int64_t
+int64_casval(int64_t* p, int64_t oldval, int64_t newval)
+{
+  return __sync_val_compare_and_swap (p, oldval, newval);
+}
+int
+int64_cas(int64_t* p, int64_t oldval, int64_t newval)
+{
+  return __sync_bool_compare_and_swap (p, oldval, newval);
+}
+#else
+/* XXX: These are not correct, but suffice for the above uses. */
+int64_t
+int64_fetch_add (int64_t* p, int64_t incr)
+{
+  int64_t t;
+  OMP("omp critical") {
+    t = *p;
+    *p += incr;
+  }
+  OMP("omp flush (p)");
+  return t;
+}
+int64_t
+int64_casval(int64_t* p, int64_t oldval, int64_t newval)
+{
+  int64_t v;
+  OMP("omp critical (CAS)") {
+    v = *p;
+    if (v == oldval)
+      *p = newval;
+  }
+  OMP("omp flush (p)");
+  return v;
+}
+int
+int64_cas(int64_t* p, int64_t oldval, int64_t newval)
+{
+  int out = 0;
+  OMP("omp critical (CAS)") {
+    int64_t v = *p;
+    if (v == oldval) {
+      *p = newval;
+      out = 1;
+    }
+  }
+  OMP("omp flush (p)");
+  return out;
+}
+#endif
+#else
+int64_t
+int64_fetch_add (int64_t* p, int64_t incr)
+{
+  int64_t t = *p;
+  *p += incr;
+  return t;
+}
+int64_t
+int64_casval(int64_t* p, int64_t oldval, int64_t newval)
+{
+  int64_t v = *p;
+  if (v == oldval)
+    *p = newval;
+  return v;
+}
+int
+int64_cas(int64_t* p, int64_t oldval, int64_t newval)
+{
+  int64_t v = *p;
+  int out = 0;
+  if (v == oldval) {
+    *p = newval;
+    out = 1;
+  }
+  return out;
+}
+#endif
diff -u /dev/null graph500-2.1.4/convey-csr/convey-csr.h
--- /dev/null	1969-12-31 18:00:00.000000000 -0600
+++ graph500-2.1.4/convey-csr/convey-csr.h	2015-03-05 10:30:14.289928000 -0600
@@ -0,0 +1,27 @@
+#include <stdlib.h>
+#include <stdint.h>
+
+#define XADJ_32
+#undef TIMER_ON
+
+#if defined (XADJ_32)
+#define XADJ_SIZE int32_t
+#else
+#define XADJ_SIZE int64_t
+#endif
+
+extern void pers_attach();
+extern void pers_detach();
+
+extern void pers_cp_free(void *ptr);
+extern void *pers_cp_malloc(size_t size);
+extern void pers_cp_memcpy(void *dst, void *src, size_t len);
+
+extern void pers_init_bfs_tree (int64_t nv, int64_t *bfs_tree);
+extern void pers_scatter_bfs (int64_t *k2, int64_t *bfs_tree,
+			      int64_t *bfs_packed);
+extern void pers_bottom_up (int64_t g500_ctl, int64_t nv,
+			    int64_t *bfs_tree, int64_t *bfs_packed_cp,
+			    XADJ_SIZE *xadj, int64_t *xoff,
+			    int64_t **bfs_tree_bit, int64_t **bfs_tree_bit_new,
+			    int64_t *k1, int64_t *k2, int64_t *oldk2);
diff -u /dev/null graph500-2.1.4/convey-csr/convey-openmp.c
--- /dev/null	1969-12-31 18:00:00.000000000 -0600
+++ graph500-2.1.4/convey-csr/convey-openmp.c	2015-03-05 10:30:14.291924000 -0600
@@ -0,0 +1,739 @@
+/* -*- mode: C; mode: folding; fill-column: 70; -*- */
+/* Copyright 2010,  Georgia Institute of Technology, USA. */
+/* See COPYING for license. */
+#include "../compat.h"
+#include "../timer.h"
+#include <stdlib.h>
+#include <stdio.h>
+#include <string.h>
+#include <unistd.h>
+#include <math.h>
+#include <xmmintrin.h>
+#include <omp.h>
+#include <semaphore.h>
+
+#include "convey-csr.h"
+
+#define MIN(a,b) ( (isnan(a)) ? (a) : ( (isnan(b)) ? (b) : ((a) < (b)) ? (a) : (b) ) )
+
+static int64_t int64_fetch_add (int64_t* p, int64_t incr);
+static int64_t int64_fetch_and (int64_t* p, int64_t incr);
+
+void loop_indices(int64_t n, int64_t *istart_for_thread, int64_t *iend_for_thread, int64_t tid, int64_t number_threads);
+
+void init_bfs_tree_wrapper ( int64_t nv, int64_t *bfs_tree);
+
+void init_m1 (int64_t nv, int64_t *bfs_tree, __m128i one128i, int64_t tid, int64_t number_threads);
+
+void top_down_openmp (int64_t nv, int64_t level, int64_t *k1, int64_t *k2, int64_t *oldk2, int64_t *bfs_packed_host, int64_t *vlist, int64_t *xoff, XADJ_SIZE *xadj, int64_t *bfs_bit_map_host, int64_t number_threads, int64_t tid);
+
+void top_down_host (int64_t nv, int64_t *level, int64_t *k1, int64_t *k2, int64_t *bfs_packed_host, int64_t *vlist, int64_t *xoff, XADJ_SIZE *xadj, int64_t *bfs_bit_map_host, int64_t number_threads, int64_t tid, int64_t bit_map_size, __m128i one128i, int64_t srcvtx, int64_t bottom_constant);
+
+double bottom_threshold();
+
+double host_total_time = 0.0;
+
+extern double walltime();
+
+extern void barrier_mk_init(void);
+extern void barrier_mk(int count);
+
+static sem_t sem1;
+static sem_t sem2;
+static sem_t sem3;
+static int my_count = 0;
+
+int64_t sixty_three =  63;
+int64_t six =  6;
+
+
+void top_hybrid (int64_t nv, int64_t srcvtx, 
+  double bottom_constant,
+  int64_t *k1, int64_t *k2,
+  int64_t *level,
+  int64_t *bfs_tree, int64_t *vlist,
+  int64_t *bfs_packed_host, int64_t *vlist_host, 
+  int64_t *xoff, XADJ_SIZE *xadj,
+  int64_t *bfs_bit_map_host,
+  int64_t *bfs_bit_map_cp)
+{
+
+// nested parallelism is not done
+#undef ENABLE_NESTING
+//#define ENABLE_NESTING
+#if defined (ENABLE_NESTING)
+// ensure nested parallelism is enabled
+//  int nest = omp_get_nested();
+//  printf ("nesting value is %d \n",nest);
+  int nest = 1;
+  omp_set_nested(nest);
+
+// ensure dynamic parallelism is enabled
+//  int dyn = omp_get_dynamic();
+//  printf ("dynamic value is %d \n",dyn);
+  int dyn = 1;
+  omp_set_dynamic(dyn);
+#endif // ENABLE_NESTING
+
+int64_t shift6 = 6;
+int64_t and63_c = 0xffffffffffffff80;
+int64_t one = 0x0000000000000001;
+
+#define BIT_LENGTH(n) ( (n) == ( (n) & and63_c) ? ((n) >> shift6 ) : ((n >> shift6 ) + one) )
+
+
+  int64_t bit_map_size;
+
+  int64_t frontier, unvisited;
+
+  int     one32 = 0xffffffff;
+  __m128i one128i;
+
+
+  int64_t number_threads, number_threads_minus_1;
+
+  double t0;
+#if defined (TIMER_ON)
+  double one_gb_pts = 1024.*1024.*1024.;
+#endif // TIMER_ON
+
+  barrier_mk_init();
+
+// all host processing
+
+// determine size of bit_map
+  bit_map_size = BIT_LENGTH(nv);
+
+  one128i = _mm_set1_epi32 (one32);
+
+  double real_host_total_time = walltime();
+
+  OMP("omp parallel shared(t0, level, bottom_constant, frontier, unvisited, k1, k2, number_threads_minus_1, number_threads, host_total_time)")
+  {
+    int64_t tid, number_threads_for_host;
+    OMP("omp single") 
+    {
+// set the number of threads to be 1 less than the maximum
+      number_threads = omp_get_num_threads();
+      number_threads_minus_1 = number_threads - 1;
+#if defined (TIMER_ON)
+      printf ("number_threads %d \n",number_threads);
+#endif // TIMER_ON
+    }
+    tid = omp_get_thread_num();
+
+///////////////////////////////////////////////////////////////////
+// code for HC-2ex
+      number_threads_for_host = number_threads_minus_1;
+      if (tid != number_threads_minus_1)
+      {
+// threads 0-(n-2) here
+          top_down_host (nv, level, k1, k2, bfs_packed_host, vlist_host, xoff, xadj, bfs_bit_map_host, number_threads_for_host, tid, bit_map_size, one128i, srcvtx, bottom_constant);
+
+// copy data to coprocessor and began CP initialization of data
+        OMP("omp single nowait") 
+          {
+
+//////////////////////////////////////////////////////////////
+// copy packed data to coprocessor
+
+#if defined (TIMER_ON)
+      double copy_bfs_vlist_bit_time;
+      t0 = walltime();
+#endif // TIMER_ON
+
+      int64_t size_bytes = sizeof(int64_t) * (bit_map_size+2*(*k2));
+      pers_cp_memcpy(bfs_bit_map_cp, bfs_bit_map_host, size_bytes );
+
+#if defined (TIMER_ON)
+      copy_bfs_vlist_bit_time = walltime() - t0;
+      printf ("copy bit map and twice k2 %ld rate (GB/s) %f time %f \n",bit_map_size+2*(*k2),(size_bytes/(one_gb_pts*copy_bfs_vlist_bit_time)),copy_bfs_vlist_bit_time);
+      host_total_time += copy_bfs_vlist_bit_time;
+      printf ("host (init+top_down+copy) time %f \n",host_total_time);
+#endif // TIMER_ON
+
+          }
+      } // end 0-(n-2) region
+    else
+      {
+#if defined (TIMER_ON)
+        double init_cp_time  = walltime();
+#endif // TIMER_ON
+
+//////////////////////////////////////////////////////////////
+// do CP processing
+        init_bfs_tree_wrapper(nv, bfs_tree);
+//////////////////////////////////////////////////////////////
+#if defined (TIMER_ON)
+        init_cp_time = walltime() - init_cp_time;
+        printf ("init CP size %ld rate (GB/s) %f time %f\n",nv,(sizeof(int64_t)*nv/(one_gb_pts*init_cp_time)),init_cp_time);
+#endif // TIMER_ON
+
+      } // end (n-1) region
+
+  } // end parallel
+
+  real_host_total_time = walltime() - real_host_total_time;
+
+  host_total_time = real_host_total_time;
+#if defined (TIMER_ON)
+  printf("The REAL time it takes for host + init is %g\n", real_host_total_time);
+  printf ("adjusted host (init+top_down+copy) time %f \n",host_total_time);
+#endif // TIMER_ON
+
+  return;
+}
+
+//////////////////////////////////////
+
+void
+top_down_openmp (int64_t nv, int64_t level, int64_t *k1, int64_t *k2, int64_t *oldk2, int64_t *bfs_packed_host, int64_t *vlist, int64_t *xoff, XADJ_SIZE *xadj, int64_t *bfs_bit_map_host, int64_t number_threads, int64_t tid)
+{
+
+#define BFS_PACKED_HOST_X(k) (bfs_packed_host[2*k])
+#define VLIST_X(k) (bfs_packed_host[2*k+1])
+
+  int64_t one = 0x0000000000000001;
+  int64_t zero  = 0x0000000000000000;
+  int64_t k;
+
+//#define THREAD_BUF_LEN_HOST 3 for debug
+#define THREAD_BUF_LEN_HOST 30000
+//#define THREAD_BUF_LEN_HOST 20000
+//#define THREAD_BUF_LEN_HOST 10000
+  int64_t nbuf[2*THREAD_BUF_LEN_HOST];
+  int64_t kbuf = 0;
+  int64_t vk;
+  int64_t word_index, bit_index, bit_integer_value;
+  int64_t v_level, bfs_j_bit_value, marked;
+
+  int64_t istart_for_thread, iend_for_thread;
+  int64_t n = *oldk2 - *k1;
+  loop_indices (n, &istart_for_thread, &iend_for_thread, tid, number_threads);
+  istart_for_thread += *k1;
+  iend_for_thread += *k1;
+
+// need barrier here on n-1 threads
+  barrier_mk(number_threads);
+
+//  printf ("n %d istart_for_thread %d iend_for_thread %d \n",n,istart_for_thread,iend_for_thread);
+//  OMP("omp for")
+
+  for (k = istart_for_thread; k < iend_for_thread; ++k) {
+//  for (k = *k1; k < *oldk2; ++k) 
+//    const int64_t v = vlist[k];
+    const int64_t v = VLIST_X(k);
+    const int64_t vso = xoff[2*v];
+    const int64_t veo = xoff[2*v+1];
+//           printf ("loop iter k %d \n",k);
+    int64_t vo;
+    v_level = v;
+    for (vo = vso; vo < veo; ++vo) {
+      const int64_t j = xadj[vo];
+// check bit value
+      word_index = j >> six;
+      bit_index = j & sixty_three;
+      bit_integer_value = ~(one << bit_index);
+
+      bfs_j_bit_value = (bfs_bit_map_host[word_index] >> bit_index) & one;
+      if ( bfs_j_bit_value != one ) continue;
+
+// do a fetch and AND
+//  if the fetched value still is unmarked then continue with the processing
+//  get value and hammer the appropriate bit - then test fetch
+        marked = int64_fetch_and (&bfs_bit_map_host[word_index], bit_integer_value);
+        if ( (marked & (~bit_integer_value) ) == zero ) continue;
+// update
+#define BETTER_PARALLEL
+//#undef BETTER_PARALLEL
+#if defined (BETTER_PARALLEL)
+        if (kbuf < THREAD_BUF_LEN_HOST) 
+          {
+// add to queue
+/*
+            nbuf_bfs_packed[kbuf] = bfs_tree[j];
+            nbuf[kbuf++] = j;
+*/
+            nbuf[2*kbuf] = v_level;
+            nbuf[2*kbuf+1] = j;
+            kbuf++;
+
+          } else {
+// empty queue
+            int64_t voff = int64_fetch_add (k2, THREAD_BUF_LEN_HOST);
+            for (vk = 0; vk < THREAD_BUF_LEN_HOST; ++vk) {
+              int64_t voff_vk = voff + vk;
+              BFS_PACKED_HOST_X(voff_vk) = nbuf[2*vk];
+              VLIST_X(voff_vk) = nbuf[2*vk+1];
+            }
+// put newest elements in queue
+            nbuf[0] = v_level;
+            nbuf[1] = j;
+            kbuf = 1;
+        }
+#else
+
+// working
+// use a thread specific temporary array to reduce number of atomic instructions
+
+        int64_t k2_temp;
+        k2_temp = int64_fetch_add (k2,one);
+// interleave these two
+/*
+        bfs_packed_host[k2_temp] = v_level;
+        vlist[k2_temp] = j;
+*/
+        BFS_PACKED_HOST_X(k2_temp) = v_level;
+        VLIST_X(k2_temp) = j;
+#endif
+
+    }
+  } // end k loop
+
+#if defined (BETTER_PARALLEL)
+// drain final elements from queue
+  if (kbuf) {
+    int64_t voff = int64_fetch_add (k2, kbuf);
+    for (vk = 0; vk < kbuf; ++vk) {
+      int64_t voff_vk = voff + vk;
+      BFS_PACKED_HOST_X(voff_vk) = nbuf[2*vk];
+      VLIST_X(voff_vk) = nbuf[2*vk+1];
+    }
+  }
+#endif
+
+  return;
+}
+//////////////////////////////////////////////////////////////////////
+void top_down_host (int64_t nv, int64_t *level, int64_t *k1, int64_t *k2, int64_t *bfs_packed_host, int64_t *vlist_host, int64_t *xoff, XADJ_SIZE *xadj, int64_t *bfs_bit_map_host, int64_t number_threads, int64_t tid, int64_t bit_map_size, __m128i one128i, int64_t srcvtx, int64_t bottom_constant)
+{
+#define BFS_PACKED_HOST_X(k) (bfs_packed_host[2*k])
+#define VLIST_X(k) (bfs_packed_host[2*k+1])
+
+  int64_t k;
+  int64_t one = 0x0000000000000001;
+
+  int64_t frontier, unvisited;
+
+#if defined (TIMER_ON)
+  double t0;
+  double one_gb_pts = 1024.*1024.*1024.;
+#endif // TIMER_ON
+
+#if defined (TIMER_ON)
+  if (tid == 0)	// use tid 0 to ensure same thread is timing
+    {
+      t0 = walltime();
+    }
+#endif // TIMER_ON
+
+  init_m1 (bit_map_size, bfs_bit_map_host, one128i, tid, number_threads);
+
+#if defined (TIMER_ON)
+  if (tid == 0)	// use tid 0 to ensure same thread is timing
+    {
+      double init_time = walltime() - t0;
+      host_total_time = init_time;
+      printf ("init host bit map size %ld rate (GB/s) %f time %f\n",bit_map_size,(sizeof(int64_t)*bit_map_size/(one_gb_pts*init_time)),init_time);
+    }
+#endif // TIMER_ON
+
+  barrier_mk(number_threads);
+
+// there are 2 sections of code that can be done in parallel
+// but running a single thread is fine
+  OMP("omp single nowait")
+    {
+////////////////////////////////////////////////////////////////
+// fix last points
+
+// finally clear out any stray points at end when not a multiple of 64
+// so want to create 0      ... 0         1       ...
+//                  off-end ... off-end  included included
+// there are
+      int64_t k;
+      int64_t word_index, bit_index, bit_integer_value, bit_scalar;
+      int64_t off_end_points = 64*bit_map_size - nv;
+//    printf ("off_end_points %d \n",off_end_points);
+      for (k=(64-off_end_points);k < 64; k++) {
+        word_index = bit_map_size-1;
+        bit_index = k;
+        bit_scalar = bfs_bit_map_host[word_index];
+        bfs_bit_map_host[word_index] = bit_scalar & (~(one << bit_index));
+      }
+
+////////////////////////////////////////////////////////////////
+// Level 1 processing for bfs, vlist, bit_map
+      *level = 1;    // note that level counting should commence with 1 not 0
+
+#define BFS_PACKED_X(k) (bfs_packed_host[2*k])
+#define VLIST_X(k) (bfs_packed_host[2*k+1])
+
+// interleave now
+      BFS_PACKED_X(0) = srcvtx; // level 1
+      VLIST_X(0) = srcvtx;
+
+// Level 1 bit_map update (does not need level number though)
+// mark first point
+      int64_t j = srcvtx; // vlist_host[0];
+      word_index = j >> six;
+      bit_index = j & sixty_three;
+      bit_integer_value = one << bit_index;
+      bfs_bit_map_host[word_index] &= (~bit_integer_value);
+//
+// Level 2 setup
+      *level = 2;
+      *k1 = 0; *k2 = 1;
+
+////////////////////////////////////////////////////////////////
+    } // end single
+    frontier = 1; unvisited = nv - 1;	// local to each thread
+    barrier_mk(number_threads);
+////////////////////////////////////////////////////////////////
+
+// do top down steps
+    while ((*k1 != *k2) && ( bottom_constant*(frontier*frontier) < unvisited ) ) {
+      int64_t oldk2 = *k2;
+//    printf ("start of while: tid %d *k1 %ld *k2 %ld oldk2 %ld level %ld \n",tid,*k1,*k2,oldk2,*level);
+#if defined (TIMER_ON)
+      if (tid == 0)	// use tid 0 to ensure same thread is timing
+        {
+          t0 = walltime();
+        }
+#endif // TIMER_ON
+
+      top_down_openmp (nv, *level, k1, k2, &oldk2, bfs_packed_host, vlist_host, xoff, xadj, bfs_bit_map_host, number_threads, tid);
+
+// need barrier
+      barrier_mk(number_threads);
+
+      OMP("omp single nowait") 
+        {
+          *k1 = oldk2;
+          *level = *level + 1;
+        }
+
+      barrier_mk(number_threads);
+
+      frontier = *k2 - *k1;
+      unvisited = nv - *k2;
+
+#if defined (TIMER_ON)
+      if (tid == 0)	// use tid 0 to ensure same thread is timing
+        {
+          double top_time = walltime() - t0;
+          host_total_time += top_time;
+          double unv_front2_ratio = (double)unvisited/(double)(frontier*frontier);
+          printf ("top_down: *k1 %ld *k2 %ld level %ld tb_ratio %f time %f\n",*k1,*k2,*level-1, unv_front2_ratio, top_time);
+          printf ("next iter will use frontier %ld unvisited %ld ratio %f\n",frontier,unvisited,unv_front2_ratio);
+        }
+#endif // TIMER_ON
+
+    }
+
+  return;
+}
+//////////////////////////////////////////////////////////////////////
+
+void
+init_m1 (int64_t n, int64_t *x, __m128i one128i, int64_t tid, int64_t number_threads)
+{
+/* Initialize n points in memory by -1 */
+
+/* if this needs to be modified for avx see
+/usr/lib/gcc/x86_64-redhat-linux/4.4.4/include/avxintrin.h
+or later
+
+#include <avxintrin.h>
+_mm256_stream_si256 (__m256i *__A, __m256i __B)
+
+
+_mm256_set1_epi64x (long long __A)
+{
+  return __extension__ (__m256i)(__v4di){ __A, __A, __A, __A };
+}
+*/
+  int64_t one64 = 0xffffffffffffffff;
+
+  int64_t thirtyone = 31;
+  int64_t three = 3;
+  int64_t unroll = 4;
+  int64_t k1;
+
+//  if (n <= 0) return;
+
+// number thread threads to use calculation done in calling routine
+
+/* want cache bypass for the stores
+
+   step to the nearest 32-byte aligned data
+     so do 1-3 point if necessary
+   do multiples of 4
+   finish (if necessary) with 1-3 more points
+
+   note that the compiler also unrolls by 4
+*/
+
+// starting point
+  int64_t offset_start, offset_end, istart;
+  int64_t istart_for_thread, iend_for_thread;
+  int64_t iend_minus_three;
+
+  loop_indices (n, &istart_for_thread, &iend_for_thread, tid, number_threads);
+
+//#define DEBUG_INITM1
+#undef DEBUG_INITM1
+#if defined (DEBUG_INITM1)
+  printf ("n %d istart_for_thread %d iend_for_thread %d \n",n,istart_for_thread,iend_for_thread);
+#endif // DEBUG_INITM1
+
+// each thread will get a section of the code to do,
+//  but then must do an appropriate alignment
+
+// hand partition the loop
+// on the HC-2ex use the first n-1 threads and not the last thread
+
+/*
+  offset_start = (thirtyone & (long)&(x[0])) >> three;
+  istart = (unroll - offset_start) % unroll;
+*/
+
+  offset_start = (thirtyone & (long)&(x[istart_for_thread])) >> three;
+  istart = istart_for_thread + (unroll - offset_start) % unroll;
+  iend_minus_three = iend_for_thread - 3;
+  offset_end = (iend_for_thread-istart) & three;
+
+//#define GENERIC_COPY
+#undef GENERIC_COPY
+#if defined (GENERIC_COPY)
+  for (k1 = istart_for_thread; k1 < iend_for_thread; k1++) {
+    x[k1] = one64;
+  }
+#else // GENERIC_COPY
+
+//  if (n <= 256)
+// must be at least 4 points per thread for unroll logic below to work
+  if ((iend_for_thread - istart_for_thread) <= 4)
+    {
+// do generic code
+      for (k1 = istart_for_thread; k1 < iend_for_thread; k1++) {
+        x[k1] = one64;
+      }
+    }
+   else
+  {
+#if defined (DEBUG_INITM1)
+  printf ("tid %d offset_start %d istart_for_thread %d offset_end %d iend_for_thread %d \n",tid,offset_start,istart_for_thread,offset_end,iend_for_thread);
+#endif // DEBUG_INITM1
+//  OMP("omp single nowait")
+  {
+
+// do 1-3 points if not 16-byte aligned
+    switch ( offset_start ) {
+      case 1:
+        x[0+istart_for_thread] = one64;
+        x[1+istart_for_thread] = one64;
+        x[2+istart_for_thread] = one64;
+#if defined (DEBUG_INITM1)
+        printf ("tid %d 3 start pts \n %d\n %d\n %d\n",tid,0+istart_for_thread, 1+istart_for_thread, 2+istart_for_thread  );
+#endif // DEBUG_INITM1
+        break;
+      case 2:
+        x[0+istart_for_thread] = one64;
+        x[1+istart_for_thread] = one64;
+#if defined (DEBUG_INITM1)
+        printf ("tid %d 2 start pts \n %d\n %d\n",tid,0+istart_for_thread, 1+istart_for_thread );
+#endif // DEBUG_INITM1
+        break;
+      case 3:
+        x[0+istart_for_thread] = one64;
+#if defined (DEBUG_INITM1)
+        printf ("tid %d 1 start pts \n %d\n",tid,0+istart_for_thread );
+#endif // DEBUG_INITM1
+        break;
+      default:
+        break;
+    }
+
+// do final 1-3 points if necessary
+    switch ( offset_end ) {
+      case 1:
+        x[iend_for_thread-1] = one64;
+#if defined (DEBUG_INITM1)
+        printf ("tid %d 1 end pts \n %d\n",tid,iend_for_thread-1 );
+#endif // DEBUG_INITM1
+        break;
+      case 2:
+        x[iend_for_thread-2] = one64;
+        x[iend_for_thread-1] = one64;
+#if defined (DEBUG_INITM1)
+        printf ("tid %d 2 end pts \n %d\n %d\n",tid,iend_for_thread-2, iend_for_thread-1 );
+#endif // DEBUG_INITM1
+        break;
+      case 3:
+        x[iend_for_thread-3] = one64;
+        x[iend_for_thread-2] = one64;
+        x[iend_for_thread-1] = one64;
+#if defined (DEBUG_INITM1)
+        printf ("tid %d 3 end pts \n %d\n %d\n %d\n",tid, iend_for_thread-3, iend_for_thread-2, iend_for_thread-1 );
+#endif // DEBUG_INITM1
+        break;
+      default:
+        break;
+    }
+
+  } // end single
+
+#if defined (DEBUG_INITM1)
+//  printf ("tid %d n %d istart %d iend_minus_three %d \n",tid,n,istart,iend_minus_three);
+#endif // DEBUG_INITM1
+//  OMP("omp for")
+    for (k1 = istart; k1 < iend_minus_three; k1+=unroll) {
+#if defined (DEBUG_INITM1)
+        printf ("tid %d in loop points \n %d\n %d\n %d\n %d\n",tid,k1,k1+1,k1+2,k1+3);
+#endif // DEBUG_INITM1
+/*
+      x[4*k1] = one64;
+      x[4*k1+1] = one64;
+      x[4*k1+2] = one64;
+      x[4*k1+3] = one64;
+*/
+// Have not seen a large performance difference between the two stores
+// for the small sizes I've looked at so far
+// Also I may want the data to reside in cache after the store
+
+// do not dirty the cache
+/*
+      _mm_stream_si128((__m128i*)&x[k1], one128i);
+      _mm_stream_si128((__m128i*)&x[k1+2], one128i);
+*/
+
+// works better for scale 24
+// dirty the cache
+      _mm_store_si128((__m128i*)&x[k1], one128i);
+      _mm_store_si128((__m128i*)&x[k1+2], one128i);
+    }
+  }
+#endif // GENERIC_COPY
+
+  return;
+}
+
+////////////////////////////////////////////////////////////////
+void loop_indices(int64_t n, int64_t *istart_for_thread, int64_t *iend_for_thread, int64_t tid, int64_t number_threads)
+{
+/*
+   return loop start and end points given the original loop size,
+   the thread id and the number of threads to use
+*/
+  int64_t n_work;
+//  n_work = (n / number_threads) + 1;
+  n_work = (n / number_threads);
+// if not exactly divisible, bump the size by 1
+  if ((number_threads * n_work) != n)
+    n_work++;
+  
+  *istart_for_thread = MIN( tid * n_work, n);
+  *iend_for_thread = MIN( (tid+1) * n_work, n);
+
+  return;
+}
+
+//////////////////////////////////////////////////////////////////////
+
+double
+bottom_threshold()
+{
+// bottom_constant allows fine tuning of the number of top down/bottom up steps
+//  start with a value of 1 since this would be correct if the
+//  relative performance of top down and bottom up are the same
+//   as bottom_constant increases more bottom up steps are done
+//
+//  bottom_constant = 1: example on s16 this gives level4:8, level3:64 steps
+//  bottom_constant = 2: example on s16 this gives level4:4, level3:63 steps
+//
+// bottom constant should be a function of top down vs. bottom up bandwidth
+//
+/*
+ Calculations for formula to determine threshold
+ host
+ HC-1/HC-1ex	bw  4 for unit stride, divide by 4.5 for gathers
+ HC-2/HC-2ex	bw 24 for unit stride, divide by 4.5 for gathers
+
+ CP
+ ISC rate	bw 30 for gather
+ July 2012	bw 50 for gather
+
+*/
+  double CP_bw = 50.0;
+  double ratio_host_unit_to_gather = 4.5;
+  double host_unit_bw = 24.0;
+
+  double host_gather_bw = host_unit_bw / ratio_host_unit_to_gather;
+  double bottom_constant = CP_bw / host_gather_bw;
+#if defined (TIMER_ON)
+  printf ("bottom_constant is %f\n", bottom_constant);
+#endif // TIMER_ON
+
+   return bottom_constant;
+}
+
+////////////////////////////////////////////////////////////////
+
+
+void barrier_mk_init(void)
+{
+  sem_init(&sem1, 0, 1);
+  sem_init(&sem2, 0, 0);
+  sem_init(&sem3, 0, 1);
+
+  return;
+}
+
+void barrier_mk(int count)
+{
+//  usleep(1000000*tid); // only for testing
+
+  sem_wait(&sem1);
+
+  my_count++;
+
+  if(my_count == count){
+    sem_wait(&sem3);
+    sem_post(&sem2);
+  }
+
+  sem_post(&sem1);
+
+  sem_wait(&sem2);
+  sem_post(&sem2);
+
+  sem_wait(&sem1);
+
+  my_count--;
+
+  if(my_count == 0){
+    sem_wait(&sem2);
+    sem_post(&sem3);
+  }
+
+  sem_post(&sem1);
+
+  sem_wait(&sem3);
+  sem_post(&sem3);
+
+  return;
+}
+////////////////////////////////////////////////////////////////
+
+int64_t
+int64_fetch_add (int64_t* p, int64_t incr)
+{
+  return __sync_fetch_and_add (p, incr);
+}
+
+int64_t
+int64_fetch_and (int64_t* p, int64_t incr)
+{
+  return __sync_fetch_and_and (p, incr);
+}
diff -u /dev/null graph500-2.1.4/convey-csr/g500_hyb_wrapper.c
--- /dev/null	1969-12-31 18:00:00.000000000 -0600
+++ graph500-2.1.4/convey-csr/g500_hyb_wrapper.c	2015-03-05 10:30:14.292924000 -0600
@@ -0,0 +1,390 @@
+/* Modified January 2011 for Convey computers */
+/*
+  Routine to perform part of Graph500 make_bfs_tree that will run on
+  coprocessor. This routine and its callees will become the pdk instruction
+*/
+
+#include <stdio.h>
+#include <stdlib.h>
+
+// for openmp definition
+#include "../compat.h"
+#include "../graph500.h"
+#include "../timer.h"
+
+#include "convey-csr.h"
+
+#undef CNY_DEBUG
+
+#define MIN(a,b) (((a) < (b)) ? (a) : (b) )
+
+void init_bfs_tree (int64_t nv, int64_t *bfs_tree);
+
+void scatter_bfs (int64_t *k2, int64_t *bfs_tree, int64_t *bfs_packed_cp);
+void scatter_check (int64_t *k2, int64_t *bfs_tree, int64_t *bfs_packed);
+
+void top_down (int64_t nv, int64_t *k1, int64_t *k2, 
+  int64_t *oldk2, int64_t *bfs_tree, int64_t *vlist, 
+  int64_t *xoff, XADJ_SIZE *xadj);
+
+void flip_addr (int64_t **a, int64_t **b);
+
+void bottom_up ( int64_t g500_ctl, int64_t nv,
+  int64_t *bfs_tree, int64_t *bfs_packed_cp, 
+  XADJ_SIZE *xadj, int64_t *xoff,
+  int64_t **bfs_tree_bit, int64_t **bfs_tree_bit_new, 
+  int64_t *k1, int64_t *k2, int64_t *oldk2);
+
+int64_t zero = 0;
+int64_t one   = 0x0000000000000001;
+int64_t one64 = 0xffffffffffffffff;   // -1
+int64_t shift6 = 6;
+int64_t and63 = 0x000000000000003f;
+int64_t and63_c = 0xffffffffffffff80;
+
+#define BIT_LENGTH(n) ( (n) == ( (n) & and63_c) ? ((n) >> shift6 ) : ((n >> shift6 ) + one) )
+
+void
+g500_hyb_wrapper ( int64_t g500_ctl, int64_t nv,
+               int64_t *bfs_tree,
+	       XADJ_SIZE *xadj, int64_t *xoff,
+               int64_t *bfs_packed_cp,
+               int64_t *bfs_bit_map_cp, int64_t *bfs_bit_map_new_cp,
+               int64_t level, int64_t *k1, int64_t *k2 )
+{
+  int use_coproc = 1;
+
+// Do bottom up algorithm on coprocessor
+
+#if defined (TIMER_ON)
+  static double coproc_scatter_time;
+  static double postproc_time;
+  static double bottom_time;
+  extern double walltime();
+  double t0;
+  int64_t size_bytes = sizeof(int64_t) * (*k2);
+  double one_gb_pts = 1024.*1024.*1024.;
+#endif // TIMER_ON
+
+  int64_t oldk2;
+
+  int64_t bit_map_size;
+  bit_map_size = BIT_LENGTH(nv);
+
+//////////////////////////////////////////////////////////////
+// Call 1 - CP instruction caep03 
+// build bfs_tree on coproc with scatter
+
+  if ( use_coproc != 0 )
+    {
+
+#if defined (SCATTER_AND_BOTTOM_UP)
+// do nothing
+
+#else
+
+#if defined (TIMER_ON)
+      t0 = walltime();
+#endif // TIMER_ON
+
+      pers_scatter_bfs (k2, bfs_tree, bfs_packed_cp);
+
+      //scatter_check (k2, bfs_tree, bfs_packed_cp);
+
+#if defined (TIMER_ON)
+      coproc_scatter_time = walltime() - t0;
+      printf ("coproc scatter size %ld rate (GB/s) %f time %f\n",*k2,3*(size_bytes/(one_gb_pts*coproc_scatter_time)),coproc_scatter_time);
+#endif // TIMER_ON
+
+#endif // SCATTER_AND_BOTTOM_UP
+    }
+  else
+    {
+      scatter_bfs (k2, bfs_tree, bfs_packed_cp);
+    }
+
+//////////////////////////////////////////////////////////////
+// Call 2 - CP instruction caep00
+//  do loop around bottom up algorithm
+      while (*k1 != *k2) {
+
+#if defined (CNY_DEBUG)
+        fprintf(stderr, "DBG:  level = %d\n",level);
+        for (int b=0; b<bit_map_size; b++) {
+          fprintf(stderr, "DBG:  bfs_bit_map_cp[%d]=0x%016llx\n",b,bfs_bit_map_cp[b]);
+        }
+#endif // CNY_DEBUG
+
+#if defined (TIMER_ON)
+        t0 = walltime();
+#endif // TIMER_ON
+
+#if defined (CNY_DEBUG)
+  fprintf(stderr, "Calling bottom up:  *k1=%ld *k2=%ld\n", *k1, *k2);
+#endif // CNY_DEBUG
+  int check=0;
+
+  int64_t *ck_bfs_tree;
+  int64_t *ck_bit_map, *ck_bit_map_new;
+  int64_t *ck_k1, *ck_k2;
+  int64_t ck_oldk2;
+  int64_t ck_k1_v, ck_k2_v;
+  ck_k1 = &ck_k1_v;
+  ck_k2 = &ck_k2_v;
+  if (check) {
+    ck_bfs_tree = pers_cp_malloc (nv * sizeof (*ck_bfs_tree));
+    ck_bit_map = pers_cp_malloc (nv/64 * sizeof (*ck_bit_map));
+    ck_bit_map_new = pers_cp_malloc (nv/64 * sizeof (*ck_bit_map_new));
+    *ck_k1 = *k1;
+    *ck_k2 = *k2;
+    for (int i=0; i<nv; i++) {
+      ck_bfs_tree[i] = bfs_tree[i];
+    }
+    for (int i=0; i<nv/64+1; i++) {
+      ck_bit_map[i] = bfs_bit_map_cp[i];
+      ck_bit_map_new[i] = bfs_bit_map_new_cp[i];
+    }
+    bottom_up ( g500_ctl, nv,
+          ck_bfs_tree, bfs_packed_cp, 
+          xadj, xoff,
+          &ck_bit_map, &ck_bit_map_new, 
+          ck_k1, ck_k2, &ck_oldk2);
+  }
+
+
+#if 0
+    for (int i=0; i<nv; i++) {
+        fprintf(stderr,"bfs_tree[%d] is %ld\n", i, bfs_tree[i]);
+    }
+#endif
+  if ( use_coproc != 0 ) {
+
+        pers_bottom_up ( g500_ctl, nv,
+
+          bfs_tree, bfs_packed_cp, 
+          xadj, xoff,
+          &bfs_bit_map_cp, &bfs_bit_map_new_cp, 
+          k1, k2, &oldk2);
+#if defined (CNY_DEBUG)
+        fprintf(stderr, "AFTER coproc call to ht_bottom_up\n");
+#endif
+  } else { // not on coprocessor
+        bottom_up ( g500_ctl, nv,
+          bfs_tree, bfs_packed_cp, 
+          xadj, xoff,
+          &bfs_bit_map_cp, &bfs_bit_map_new_cp, 
+          k1, k2, &oldk2);
+  }
+
+  if (check) {
+    if (*ck_k1 != *k1) {
+      fprintf(stderr, "ERROR:  ck_k1 (%lld) != k1 (%lld)\n", *ck_k1, *k1);
+    }
+    if (*ck_k2 != *k2) {
+      fprintf(stderr, "ERROR:  ck_k2 (%lld) != k2 (%lld)\n", *ck_k2, *k2);
+    }
+    if (ck_oldk2 != oldk2) {
+      fprintf(stderr, "ERROR:  ck_oldk2 (%lld) != oldk2 (%lld)\n", ck_oldk2, oldk2);
+    }
+    for (int i=0; i<nv; i++) {
+      if (ck_bfs_tree[i] != bfs_tree[i])
+        fprintf(stderr, "ERROR:  ck_bfs_tree[%d] (0x%llx) != bfs_tree[%d] (0x%llx)\n", i, ck_bfs_tree[i], i, bfs_tree[i]);
+    }
+    for (int i=0; i<nv/64+1; i++) {
+      if (ck_bit_map[i] != bfs_bit_map_cp[i]) {
+        fprintf(stderr, "ERROR:  ck_bit_map[i] (0x%llx) != bfs_bit_map_cp[i] (0x%llx)\n", i, ck_bit_map[i], i, bfs_bit_map_cp[i]);
+      }
+      if (ck_bit_map_new[i] != bfs_bit_map_new_cp[i]) {
+        fprintf(stderr, "ERROR:  ck_bit_map_new[i] (0x%llx) != bfs_bit_map_new_cp[i] (0x%llx)\n", i, ck_bit_map_new[i], i, bfs_bit_map_new_cp[i]);
+      }
+    }
+  }
+
+#if defined (CNY_DEBUG)
+  fprintf(stderr, "After bottom up:  *k1=%ld *k2=%ld\n", *k1, *k2);
+#endif // CNY_DEBUG
+
+#if defined (CNY_DEBUG)
+for (int i=0;i<nv;i++)
+  fprintf(stderr, "xbfs_tree[%d]=%llx (0x%llx)\n", i, bfs_tree[i], &bfs_tree[i]);
+
+  for (int ixx=0;ixx<nv/64+1;ixx++){
+    fprintf (stderr, "ixx %d bit maps %lx %lx \n",ixx,bfs_bit_map_cp[ixx],bfs_bit_map_new_cp[ixx]);
+  }
+#endif // CNY_DEBUG
+
+#if defined (TIMER_ON)
+        bottom_time = walltime() - t0;
+        printf ("bottom_up: *k2 %ld level %ld time %f\n", *k2, level, bottom_time);
+        level++;
+#endif // TIMER_ON
+      } // end while - while is sometimes in inlined code
+
+//////////////////////////////////////////////////////////////
+
+  return;
+}
+
+void
+init_bfs_tree (int64_t nv, int64_t *bfs_tree)
+{
+  int64_t k1;
+  for (k1 = 0; k1 < nv; ++k1)
+    bfs_tree[k1] = one64;
+
+  return;
+}
+
+void
+top_down (int64_t nv, int64_t *k1, int64_t *k2, int64_t *oldk2, int64_t *bfs_tree, int64_t *vlist, int64_t *xoff, XADJ_SIZE *xadj)
+{
+  int64_t k;
+  for (k = *k1; k < *oldk2; ++k) {
+    const int64_t v = vlist[k];
+    const int64_t vso = xoff[2*v];
+    const int64_t veo = xoff[2*v+1];
+    int64_t vo;
+    for (vo = vso; vo < veo; ++vo) {
+      const int64_t j = xadj[vo];
+      if (bfs_tree[j] == one64) {
+        bfs_tree[j] = v;
+        vlist[(*k2)++] = j;
+      }
+    }
+  }
+  *k1 = *oldk2;
+  return;
+}
+
+void
+bottom_up ( int64_t g500_ctl, int64_t nv,
+   int64_t *bfs_tree, int64_t *bfs_packed_cp, 
+   XADJ_SIZE *xadj, int64_t *xoff,
+   int64_t **bfs_tree_bit, int64_t **bfs_tree_bit_new,
+   int64_t *k1, int64_t *k2, int64_t *oldk2)
+{
+  int64_t i, i_out;
+  int64_t bit_scalar, bit_working, bit_extract;
+  int64_t i_out_mod64, i_out_div64;
+  int64_t *temp;
+
+// split nv over the number of function pipes in chunks of 64
+//
+  *oldk2 = *k2;
+
+  fprintf(stderr, "bottom_up:  k1=%lld k2=%lld oldk2=%lld\n", (long long)*k1, (long long)*k2, (long long)*oldk2);
+
+
+  int64_t i_out_div64_neighbor, bit_scalar_neighbor;
+  int64_t i_out_mod64_neighbor, bit_extract_neighbor;
+
+  for (i_out = 0; i_out < nv; i_out+=64) {
+    i_out_div64 = i_out / 64;
+
+    bit_scalar = (*bfs_tree_bit)[i_out_div64];
+    bit_working = bit_scalar;
+
+      for (i = i_out; i < MIN(i_out+64,nv); i++) {
+
+//  do 64 at a time per pipe,
+//    to reduce bfs_tree loads by factor of 64
+//    since all final steps are bottom_up can delete vlist store, but still need k2 incr
+//
+        i_out_mod64 = i - i_out;
+        bit_extract = ((bit_scalar >> i_out_mod64) & one);
+        if (bit_extract == one) {
+
+          const int64_t vso = xoff[2*i];
+          const int64_t veo = xoff[2*i+1];
+          int64_t vo;
+
+// if a vertex has no connection, mark it as updated in bfs_tree_bit
+          if ( (veo - vso) <= 0 )
+            {
+                bit_working = bit_working & ( (one << i_out_mod64) ^ one64); // clear done ones
+                (*k2)++;
+                goto nexti;
+            }
+
+// each pipe does for loop
+          for (vo = vso; vo < veo; ++vo) {
+            const int64_t neighbor = xadj[vo];
+
+// is neighbor in frontier?
+// check old bit map instead of level
+            i_out_div64_neighbor = neighbor / 64;
+            i_out_mod64_neighbor = neighbor % 64;
+            bit_scalar_neighbor = (*bfs_tree_bit)[i_out_div64_neighbor];
+//fprintf (stderr, "div64_neighbor=%d mod64_neighbor=%d bit_scalar_neighbor=%d\n", i_out_div64_neighbor, i_out_mod64_neighbor, bit_scalar_neighbor);
+            bit_extract_neighbor = ((bit_scalar_neighbor >> i_out_mod64_neighbor) & one);
+//fprintf(stderr, "bottom_up:  i=%d vo=%d neighbor=%lld bmap[%d] bit=%d bit_working=%llx\n", i, vo, neighbor, i_out_div64_neighbor, i_out_mod64_neighbor, bit_working);
+            if (bit_extract_neighbor == zero)
+              {
+                bfs_tree[i] = neighbor; // have each pipe write this
+                bit_working = bit_working & ( (one << i_out_mod64) ^ one64); // clear done ones
+
+//              vlist[(*k2)++] = i;     // pass i to AE0 or pass k2 between AE?
+                (*k2)++;
+#if defined (CNY_DEBUG)
+  fprintf(stderr, "bottom_up:  Updated bmapIdx[%d] bit %d, neighbor is bmap[%d] bit %d, *k2=%lld\n", i_out_div64, i_out_mod64, i_out_div64_neighbor, i_out_mod64_neighbor, *k2);
+  fprintf(stderr, "bottom_up:  bfs_tree[%d]=%lld \n", i, bfs_tree[i]);
+#endif // CNY_DEBUG
+                goto nexti;
+              }
+          }
+        }
+       else
+// already updated
+        {
+          bit_working = bit_working & ( (one << i_out_mod64) ^ one64); // clear done ones
+        }
+nexti:
+        continue;
+      }
+    (*bfs_tree_bit_new)[i_out_div64] = bit_working;
+  }
+
+  *k1 = *oldk2;
+
+  fprintf(stderr, "bottom_up return:  k1=%lld k2=%lld oldk2=%lld\n", (long long)*k1, (long long)*k2, (long long)*oldk2);
+
+////////////////////////////////////////////////////////////////
+// flip addresses - see separate function
+////////////////////////////////////////////////////////////////
+  temp = *bfs_tree_bit;
+  *bfs_tree_bit = *bfs_tree_bit_new;
+  *bfs_tree_bit_new = temp;
+
+  return;
+}
+
+void
+scatter_bfs (int64_t *k2, int64_t *bfs_tree, int64_t *bfs_packed)
+{
+  int64_t k;
+
+#define BFS_PACKED_X(k) (bfs_packed[2*k])
+#define VLIST_X(k) (bfs_packed[2*k+1])
+
+  for (k=0; k<(*k2); k++) {
+//    bfs_tree[vlist[k]] = bfs_packed[k];
+    bfs_tree[VLIST_X(k)] = BFS_PACKED_X(k);
+  }
+  return;
+}
+
+void
+scatter_check (int64_t *k2, int64_t *bfs_tree, int64_t *bfs_packed)
+{
+  int64_t k;
+
+#define BFS_PACKED_X(k) (bfs_packed[2*k])
+#define VLIST_X(k) (bfs_packed[2*k+1])
+
+  for (k=0; k<(*k2); k++) {
+    if (bfs_tree[VLIST_X(k)] != BFS_PACKED_X(k))
+      fprintf(stderr, "ERROR:  bfs_tree[%d] (0x%llx) = 0x%llx , expected 0x%llx\n", 
+                               VLIST_X(k), &bfs_tree[VLIST_X(k)], bfs_tree[VLIST_X(k)], BFS_PACKED_X(k));
+  }
+  return;
+}
diff -u /dev/null graph500-2.1.4/convey-csr/init_bfs_tree_wrapper.c
--- /dev/null	1969-12-31 18:00:00.000000000 -0600
+++ graph500-2.1.4/convey-csr/init_bfs_tree_wrapper.c	2015-03-05 10:30:14.293926000 -0600
@@ -0,0 +1,8 @@
+#include "convey-csr.h"
+
+void
+init_bfs_tree_wrapper(int64_t nv, int64_t *bfs_tree)
+{
+  pers_init_bfs_tree (nv, bfs_tree);
+  return;
+}
diff -u /dev/null graph500-2.1.4/convey-csr/walltime.c
--- /dev/null	1969-12-31 18:00:00.000000000 -0600
+++ graph500-2.1.4/convey-csr/walltime.c	2015-03-05 10:30:14.294925000 -0600
@@ -0,0 +1,17 @@
+// walltime.c - return walltime
+#include <stdio.h>
+#include <stdlib.h>
+#include <sys/time.h>
+
+double walltime ()
+{
+  double mic, time;
+  double mega = 0.000001;
+  struct timeval tp;
+//  struct timezone tzp;
+  (void) gettimeofday(&tp,NULL);
+  time = (double) (tp.tv_sec);
+  mic  = (double) (tp.tv_usec);
+  time = (time + mic * mega);
+  return(time);
+}
diff -u graph500-2.1.4.orig/graph500.c graph500-2.1.4/graph500.c
--- graph500-2.1.4.orig/graph500.c	2011-05-12 13:48:26.000000000 -0500
+++ graph500-2.1.4/graph500.c	2015-03-05 10:30:14.349926000 -0600
@@ -31,6 +31,10 @@
 #include "generator/graph_generator.h"
 #include "generator/make_graph.h"
 
+#ifdef CONVEY
+#include "convey-csr/convey-csr.h"
+#endif
+
 static int64_t nvtx_scale;
 
 static int64_t bfs_root[NBFS_max];
@@ -111,6 +115,9 @@
   output_results (SCALE, nvtx_scale, edgefactor, A, B, C, D,
 		  generation_time, construction_time, NBFS, bfs_time, bfs_nedge);
 
+#ifdef CONVEY
+  pers_detach();
+#endif
   return EXIT_SUCCESS;
 }
 
@@ -136,8 +143,10 @@
   */
   if (!rootname) {
     has_adj = xmalloc_large (nvtx_scale * sizeof (*has_adj));
+#ifndef CONVEY
     OMP("omp parallel") {
       OMP("omp for")
+#endif
 	for (k = 0; k < nvtx_scale; ++k)
 	  has_adj[k] = 0;
       MTA("mta assert nodep") OMP("omp for")
@@ -147,7 +156,9 @@
 	  if (i != j)
 	    has_adj[i] = has_adj[j] = 1;
 	}
+#ifndef CONVEY
     }
+#endif
 
     /* Sample from {0, ..., nvtx_scale-1} without replacement. */
     m = 0;
@@ -184,15 +195,34 @@
     close (fd);
   }
 
+#ifdef CONVEY
+  pers_attach(); 
+
+  // (bfs_tree_packed + vlist_packed) + bit_map (map maps requires 1/64 space)
+  int64_t host_points = (2.02 * (double)nvtx_scale);
+  // bfs_tree + (bfs_tree_packed + vlist_packed) + 2 x bit_map
+  int64_t CP_points = (3.05 * (double)nvtx_scale);
+
+  int64_t *bfs_tree, *cny_bfs_tree, *verify_edge_buf;
+  bfs_tree = xmalloc_large (host_points * sizeof (*bfs_tree));
+  cny_bfs_tree = pers_cp_malloc (CP_points * sizeof (*cny_bfs_tree));
+  verify_edge_buf = xmalloc_large(2*nvtx_scale * sizeof(*verify_edge_buf));
+#else
+  int64_t *bfs_tree;
+  bfs_tree = xmalloc_large (nvtx_scale * sizeof (*bfs_tree));
+#endif
+
   for (m = 0; m < NBFS; ++m) {
-    int64_t *bfs_tree, max_bfsvtx;
+    int64_t max_bfsvtx;
 
-    /* Re-allocate. Some systems may randomize the addres... */
-    bfs_tree = xmalloc_large (nvtx_scale * sizeof (*bfs_tree));
     assert (bfs_root[m] < nvtx_scale);
 
     if (VERBOSE) fprintf (stderr, "Running bfs %d...", m);
+#ifdef CONVEY
+    TIME(bfs_time[m], err = make_bfs_tree (bfs_tree, cny_bfs_tree, &max_bfsvtx, bfs_root[m]));
+#else
     TIME(bfs_time[m], err = make_bfs_tree (bfs_tree, &max_bfsvtx, bfs_root[m]));
+#endif
     if (VERBOSE) fprintf (stderr, "done\n");
 
     if (err) {
@@ -200,18 +230,36 @@
       abort ();
     }
 
+#ifdef CONVEY
+   // copy data to host for more efficient verify routine
+   pers_cp_memcpy (bfs_tree, cny_bfs_tree, nvtx_scale * sizeof (*cny_bfs_tree));
+#endif
+
     if (VERBOSE) fprintf (stderr, "Verifying bfs %d...", m);
+#ifdef CONVEY
+    bfs_nedge[m] = verify_bfs_tree (bfs_tree, max_bfsvtx, bfs_root[m], IJ, nedge, verify_edge_buf);
+#else
     bfs_nedge[m] = verify_bfs_tree (bfs_tree, max_bfsvtx, bfs_root[m], IJ, nedge);
+#endif
     if (VERBOSE) fprintf (stderr, "done\n");
     if (bfs_nedge[m] < 0) {
       fprintf (stderr, "bfs %d from %" PRId64 " failed verification (%" PRId64 ")\n",
 	       m, bfs_root[m], bfs_nedge[m]);
       abort ();
     }
-
-    xfree_large (bfs_tree);
+#if defined(CONVEY) && defined(TIMER_ON)
+    printf ("bfs_time %f bfs_nedge %ld rate %ld \n",bfs_time[m],bfs_nedge[m],(long)((double)bfs_nedge[m]/(double)bfs_time[m]));
+    fflush(stdout);
+#endif
   }
 
+  xfree_large (bfs_tree);
+
+#ifdef CONVEY
+  xfree_large (verify_edge_buf);
+  pers_cp_free (cny_bfs_tree);
+#endif
+
   destroy_graph ();
 }
 
diff -u graph500-2.1.4.orig/graph500.h graph500-2.1.4/graph500.h
--- graph500-2.1.4.orig/graph500.h	2011-05-12 13:48:26.000000000 -0500
+++ graph500-2.1.4/graph500.h	2015-03-05 10:30:14.350928000 -0600
@@ -13,8 +13,14 @@
 int create_graph_from_edgelist (struct packed_edge *IJ, int64_t nedge);
 
 /** Create the BFS tree from a given source vertex. */
+#ifdef CONVEY
+int make_bfs_tree (int64_t *bfs_tree_out_host,
+		   int64_t *bfs_tree_out, int64_t *max_vtx_out,
+		   int64_t srcvtx);
+#else
 int make_bfs_tree (int64_t *bfs_tree_out, int64_t *max_vtx_out,
 		   int64_t srcvtx);
+#endif
 
 /** Clean up. */
 void destroy_graph (void);
diff -u graph500-2.1.4.orig/Makefile graph500-2.1.4/Makefile
--- graph500-2.1.4.orig/Makefile	2011-05-12 13:48:26.000000000 -0500
+++ graph500-2.1.4/Makefile	2015-03-05 10:30:14.284927000 -0600
@@ -3,7 +3,8 @@
 # See COPYING for license.
 BUILD_OPENMP = No
 BUILD_XMT = No
-include make.inc
+BUILD_CONVEY = Yes
+include make-incs/make.inc-convey
 
 GRAPH500_SOURCES=graph500.c options.c rmat.c kronecker.c verify.c prng.c \
 	xalloc.c timer.c 
@@ -25,6 +26,10 @@
 BIN = xmt-csr/xmt-csr xmt-csr-local/xmt-csr-local
 endif
 
+ifeq ($(BUILD_CONVEY), Yes)
+BIN = convey-csr/convey-csr
+endif
+
 GENERATOR_SRCS=splittable_mrg.c	\
 	graph_generator.c \
 	make_graph.c utils.c
@@ -34,12 +39,14 @@
 
 CPPFLAGS += -I./generator
 
-make-edgelist: CFLAGS:=$(CFLAGS) $(CFLAGS_OPENMP)
+make-edgelist: CFLAGS:=$(CFLAGS) $(CFLAGS_OPENMP) $(DEFINE)
 make-edgelist:	make-edgelist.c options.c rmat.c kronecker.c prng.c \
 	xalloc.c timer.c $(addprefix generator/,$(GENERATOR_SRCS))
 
+seq-list/seq-list: CFLAGS:=$(CFLAGS) $(DEFINE)
 seq-list/seq-list: seq-list/seq-list.c $(GRAPH500_SOURCES) \
 	$(addprefix generator/,$(GENERATOR_SRCS))
+seq-csr/seq-csr: CFLAGS:=$(CFLAGS) $(DEFINE)
 seq-csr/seq-csr: seq-csr/seq-csr.c $(GRAPH500_SOURCES) \
 	$(addprefix generator/,$(GENERATOR_SRCS))
 
@@ -55,6 +62,35 @@
 xmt-csr-local/xmt-csr-local: xmt-csr-local/xmt-csr-local.c $(GRAPH500_SOURCES) \
 	$(addprefix generator/,$(GENERATOR_SRCS))
 
+LIB_HT ?= ../lib_pers/libhtapp.a
+PDK_CFG = $(wildcard $(dir $(LIB_HT))ht/config.pdk)
+ifneq ($(PDK_CFG),)
+PDK_PLATFORM = $(shell grep 'PLATFORM =' $(PDK_CFG) | sed 's/^.*= //')
+endif
+
+convey-csr/convey-csr: CFLAGS:=$(CFLAGS) -DCONVEY $(DEFINE) $(CFLAGS_OPENMP)
+convey-csr/convey-csr: LDLIBS += $(LIB_HT)
+
+ifeq ($(findstring sysc,$(LIB_HT)),)
+ ifneq ($(findstring hc,$(PDK_PLATFORM)),)
+  convey-csr/convey-csr: LDLIBS += -L/opt/convey/lib
+  convey-csr/convey-csr: LDLIBS += /opt/convey/lib/cny_initruntime.o
+  convey-csr/convey-csr: LDLIBS += -lcny_utils -lPersSupport -ldl
+ else
+  ifneq ($(findstring vsim,$(LIB_HT)),)
+   convey-csr/convey-csr: LDLIBS += -L /opt/convey/lib -lwx_sim_runtime
+  else
+   convey-csr/convey-csr: LDLIBS += -L /opt/convey/lib -lwx_runtime
+  endif
+ endif
+endif
+
+convey-csr_src = $(wildcard convey-csr/*.c) \
+	$(GRAPH500_SOURCES) $(addprefix generator/,$(GENERATOR_SRCS))
+convey-csr_objs = $(convey-csr_src:.c=.o)
+convey-csr/convey-csr: $(convey-csr_objs) $(LIB_HT)
+	$(CXX) $(CFLAGS) $(convey-csr_objs) -o $@  $(LDLIBS)
+
 generator/generator_test_seq: generator/generator_test_seq.c $(GENERATOR_SRCS)
 
 generator/generator_test_omp: generator/generator_test_omp.c $(GENERATOR_SRCS)
@@ -65,5 +101,5 @@
 .PHONY:	clean
 clean:
 	rm -f generator/generator_test_seq generator/generator_test_omp \
-		$(BIN)
+		$(BIN) $(convey-csr_objs)
 	-$(MAKE) -C mpi clean
diff -u /dev/null graph500-2.1.4/make-incs/make.inc-convey
--- /dev/null	1969-12-31 18:00:00.000000000 -0600
+++ graph500-2.1.4/make-incs/make.inc-convey	2015-03-05 10:30:14.355926000 -0600
@@ -0,0 +1,5 @@
+CFLAGS = -g -O3 -std=c99
+LDLIBS = -lm -lrt
+
+BUILD_OPENMP = Yes
+CFLAGS_OPENMP = -fopenmp
diff -u graph500-2.1.4.orig/options.h graph500-2.1.4/options.h
--- graph500-2.1.4.orig/options.h	2011-05-12 13:48:26.000000000 -0500
+++ graph500-2.1.4/options.h	2015-03-05 10:30:14.447925000 -0600
@@ -16,7 +16,9 @@
 
 extern double A, B, C, D;
 
+#ifndef NBFS_max
 #define NBFS_max 64
+#endif
 extern int NBFS;
 
 #define default_SCALE ((int64_t)14)
diff -u graph500-2.1.4.orig/verify.c graph500-2.1.4/verify.c
--- graph500-2.1.4.orig/verify.c	2011-05-12 13:48:26.000000000 -0500
+++ graph500-2.1.4/verify.c	2015-03-05 10:30:14.468926000 -0600
@@ -78,7 +78,11 @@
 int64_t
 verify_bfs_tree (int64_t *bfs_tree_in, int64_t max_bfsvtx,
 		 int64_t root,
-		 const int64_t *IJ_in, int64_t nedge)
+		 const int64_t *IJ_in, int64_t nedge
+#ifdef CONVEY
+		 , int64_t *edge_buf
+#endif
+		 )
 {
   int64_t * restrict bfs_tree = bfs_tree_in;
   const int64_t * restrict IJ = IJ_in;
@@ -98,7 +102,11 @@
 
   err = 0;
   nedge_traversed = 0;
+#ifdef CONVEY
+  seen_edge = edge_buf;
+#else
   seen_edge = xmalloc_large (2 * (nv) * sizeof (*seen_edge));
+#endif
   level = &seen_edge[nv];
 
   err = compute_levels (level, nv, bfs_tree, root);
@@ -171,7 +179,9 @@
   }
  done:
 
+#ifndef CONVEY
   xfree_large (seen_edge);
+#endif
   if (err) return err;
   return nedge_traversed;
 }
diff -u graph500-2.1.4.orig/verify.h graph500-2.1.4/verify.h
--- graph500-2.1.4.orig/verify.h	2011-05-12 13:48:26.000000000 -0500
+++ graph500-2.1.4/verify.h	2015-03-05 10:30:14.469927000 -0600
@@ -9,6 +9,10 @@
 /** Verify a BFS tree, return volume or -1 if failed. */
 int64_t verify_bfs_tree (int64_t *bfs_tree, int64_t max_bfsvtx,
 			 int64_t root,
-			 const struct packed_edge *IJ, int64_t nedge);
+			 const struct packed_edge *IJ, int64_t nedge
+#ifdef CONVEY
+			 , int64_t *edge_buf
+#endif
+			 );
 
 #endif /* VERIFY_HEADER_ */
